{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "곰돌이 푸, 행복한 일은 매일 있어\n",
      "모든 순간이 너였다\n",
      "무례한 사람에게 웃으며 대처하는 법\n",
      "82년생 김지영(오늘의 젊은 작가 13)(양장본 HardCover)\n",
      "나는 나로 살기로 했다\n",
      "언어의 온도(3주년 150만부 기념 에디션)\n",
      "죽고 싶지만 떡볶이는 먹고 싶어\n",
      "돌이킬 수 없는 약속\n",
      "나미야 잡화점의 기적(양장본 HardCover)\n",
      "역사의 역사\n",
      "신경 끄기의 기술\n",
      "말 그릇(비울수록 사람을 더 채우는)\n",
      "개인주의자 선언\n",
      "어떻게 살 것인가\n",
      "3층 서기실의 암호\n",
      "열두 발자국\n",
      "말의 품격\n",
      "곰돌이 푸, 서두르지 않아도 괜찮아\n",
      "연애의 행방\n",
      "사피엔스\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1) reqeusts 라이브러리를 활용한 HTML 페이지 요청 \n",
    "# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출할 수 있음\n",
    "res = requests.get('http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?range=1&kind=3&orderClick=DAC&mallGb=KOR&linkClass=A')\n",
    "\n",
    "# print(res.content)\n",
    "# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법)\n",
    "# 2-1) BeautifulSoup 파싱방법\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "contents = soup.find(id='main_contents')\n",
    "\n",
    "# 3) 필요한 데이터 검색\n",
    "for title in contents.find_all(class_='title'):\n",
    "\n",
    "    # 4) 필요한 데이터 추출\n",
    "    print(title.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ad9d653dba73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mArtdayAuctionSpider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'artday-auction'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     start_urls = [\n\u001b[0;32m      4\u001b[0m         \u001b[1;34mf'http://www.artday.co.kr/pages/auction/online-auction.php?page={i}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e2613e357d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1) reqeusts 라이브러리를 활용한 HTML 페이지 요청 \n",
    "# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출할 수 있음\n",
    "res = requests.get('http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?range=1&kind=3&orderClick=DAC&mallGb=KOR&linkClass=A')\n",
    "\n",
    "# print(res.content)\n",
    "# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법)\n",
    "# 2-1) BeautifulSoup 파싱방법\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "contents = soup.find(id='main_contents')\n",
    "\n",
    "# 3) 필요한 데이터 검색\n",
    "for title in contents.find_all(class_='title'):\n",
    "\n",
    "    # 4) 필요한 데이터 추출\n",
    "    print(title.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-ffd7ff5198f2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-ffd7ff5198f2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    scrapy shell \"http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?range=1&kind=3&orderClick=DAC&mallGb=KOR&linkClass=A\"\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from austmps.items import AustmpsItem # We need this so that Python knows about the item object\n",
    "\n",
    "class AustmpdataSpider(scrapy.Spider):\n",
    "    name = 'austmpdata'  # The name of this spider\n",
    "    \n",
    "    # The allowed domain and the URLs where the spider should start crawling:\n",
    "    allowed_domains = ['www.kyobobook.co.kr']\n",
    "    start_urls = ['http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?range=1&kind=3&orderClick=DAC&mallGb=KOR&linkClass=A']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # The main method of the spider. It scrapes the URL(s) specified in the\n",
    "        # 'start_url' argument above. The content of the scraped URL is passed on\n",
    "        # as the 'response' object.\n",
    "\n",
    "        # the function is an iterator, so we need to iterate over it. There is likely a cleaner way to do this.\n",
    "        for item in self.scrape(response):\n",
    "            yield item\n",
    "\n",
    "    def scrape(self, response):\n",
    "        for resource in response.xpath(\"//h4[@class='title']/..\"):\n",
    "            # Loop over each item on the page. \n",
    "            item = AustmpsItem() # Creating a new Item object\n",
    "\n",
    "            item['name'] = resource.xpath(\"h4/a/text()\").extract_first()\n",
    "            item['link'] = resource.xpath(\"h4/a/@href\").extract_first()\n",
    "            item['district'] = resource.xpath(\"dl/dd/text()\").extract_first()\n",
    "            item['twitter'] = resource.xpath(\"dl/dd/a[contains(@class, 'twitter')]/@href\").extract_first()\n",
    "            item['party'] = resource.xpath(\"dl/dt[text()='Party']/following-sibling::dd/text()\").extract_first()\n",
    "\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
